#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TWSE æ¸…ç†å™¨ï¼šåˆä½µè‡ª data_cleaning/cleaner_main.py + cleaning_utils.py

ç‰¹æ€§
- å¯ CLI èˆ‡ import é›™ç”¨
- é è¨­ DEBUG loggingï¼ˆä¸éœ€è¦ --debugï¼‰
- åš´æ ¼éŒ¯èª¤ç­–ç•¥ï¼šæ¸…ç†éç¨‹é‡éŒ¯ã€Œç«‹åˆ»ä¸­æ­¢ã€ï¼Œä¸è·³éã€ä¸å¯« .csv/.txtï¼›æŠŠå‡ºéŒ¯çš„è³‡æ–™å€¼/å‹åˆ¥ã€æ¬„ä½ã€å­è¡¨ã€æª”æ¡ˆç­‰è©³åˆ—ï¼Œæ–¹ä¾¿ä½ ç«‹å³è£œæ¸…ç†è¦å‰‡
- æ¯å€‹å‡½å¼å¯å–®ç¨æ¸¬è©¦
- é‡è¤‡/å†—é¤˜é‚è¼¯å·²ç²¾ç°¡ï¼ˆæ¬„åæ­¸ä¸€è™•ç†ç­‰ï¼‰


ç”¨æ³•ï¼ˆCLIï¼‰
    æ¸…ç†æ‰€æœ‰é¡åˆ¥ï¼š
        python -m data_cleaning.twse
    åªæ¸…ç†æŒ‡å®šé¡åˆ¥ï¼ˆè³‡æ–™å¤¾å/collection keyï¼‰ï¼š
        python -m data_cleaning.twse --col æ¯æ—¥æ”¶ç›¤è¡Œæƒ… ä¿¡ç”¨äº¤æ˜“çµ±è¨ˆ

ç”¨æ³•ï¼ˆimportï¼‰
    from data_cleaning.twse import process_twse_data, clean_one_dataframe
    process_twse_data(["æ¯æ—¥æ”¶ç›¤è¡Œæƒ…"])
    # æˆ–å–®æ¸¬ï¼š
    df2 = clean_one_dataframe(df_raw, item="æ¯æ—¥æ”¶ç›¤è¡Œæƒ…", subitem="å€‹è‚¡", date="2025-08-10")

    from data_cleaning.twse import process_twse_data

    # æ¸…å…¨éƒ¨
    process_twse_data()

    # æˆ–åªæ¸…ç‰¹å®šé›†åˆ
    process_twse_data(["æ¯æ—¥æ”¶ç›¤è¡Œæƒ…"])

    import pandas as pd
    from data_cleaning.twse import clean_one_dataframe

    # å‡è³‡æ–™ï¼šæ¬„åæœƒå…ˆç¶“é colname_dic èˆ‡ HTML æ¸…ç†
    raw = pd.DataFrame(
        [["114/08/05", "2330", "10,000", "1,234.5"]],
        columns=["æ—¥æœŸ", "ä»£è™Ÿ", "æˆäº¤è‚¡æ•¸", "æ”¶ç›¤åƒ¹</br>"]
    )

    cleaned = clean_one_dataframe(
        raw,
        item="æ¯æ—¥æ”¶ç›¤è¡Œæƒ…",
        subitem="å€‹è‚¡",
        date="2025-08-10"  # è‹¥ç„¡æ—¥æœŸæ¬„ï¼Œæœƒç”¨é€™å€‹è£œ 'date'
    )
    print(cleaned.dtypes)
    print(cleaned.head())


"""
import argparse
import logging
from os import makedirs
from os.path import exists, join, splitext, basename
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime

import pandas as pd

# ---- StevenTricks èˆ‡ config ----
from StevenTricks.io.file_utils import pickleio
from StevenTricks.io.file_utils import PathWalk_df

from StevenTricks.core.convert_utils import safe_replace, safe_numeric_convert, stringtodate, keyinstr
from StevenTricks.db.internal_db import DBPkl

from config.conf import collection, fields_span, dropcol, key_set
from config.col_rename import colname_dic, transtonew_col
from config.col_format import numericol, datecol
from config.paths import (
    db_local_root,
)
# ---- é›²ç«¯ / æœ¬æ©Ÿ è·¯å¾‘å¿«å– ----
# ä¸€é–‹å§‹è¼‰å…¥æ™‚ï¼Œconfig.paths çš„ db_root æŒ‡å‘ã€Œé›²ç«¯ã€ï¼Œæ‰€ä»¥é€™ä¸‰å€‹å°±æ˜¯é›²ç«¯è·¯å¾‘
CLOUD_DBPATH_SOURCE = dbpath_source
CLOUD_DBPATH_CLEANED = dbpath_cleaned
CLOUD_DBPATH_CLEANED_LOG = dbpath_cleaned_log

# æœ¬æ©Ÿæ ¹ç›®éŒ„ï¼ˆconfig.paths.path_dic["stock_twse_db"]["db_local"]ï¼‰
LOCAL_DB_ROOT = db_local_root
LOCAL_DBPATH_SOURCE = LOCAL_DB_ROOT / "source"
LOCAL_DBPATH_CLEANED = LOCAL_DB_ROOT / "cleaned"
LOCAL_DBPATH_CLEANED_LOG = LOCAL_DBPATH_CLEANED / "log.pkl"

from StevenTricks.io.staging import staging_path

DEBUG_LAST_DF: Optional[pd.DataFrame] = None
DEBUG_LAST_CONTEXT: Dict[str, Any] = {}

# ---- Loggingï¼šé è¨­ DEBUG ----
_root = logging.getLogger()
if not _root.handlers:
    logging.basicConfig(
        level=logging.DEBUG,
        format="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

# ---- job_state è¨­è¨ˆ ----
# çµ±ä¸€ job_state çš„æ¬„ä½ï¼Œä¸è«–æ–°èˆŠç‰ˆæœ¬éƒ½èµ°é€™ä¸€å¥—
JOB_STATE_COLUMNS = [
    "file",              # æª”åï¼ˆä¸å«è·¯å¾‘ï¼‰
    "path",              # å®Œæ•´è·¯å¾‘
    "dir",               # ä¸Šå±¤è³‡æ–™å¤¾ï¼ˆTWSE é¡åˆ¥ï¼Œä¾‹å¦‚ ä¸‰å¤§æ³•äººè²·è³£è¶…æ—¥å ±ï¼‰
    "hash",              # æª”æ¡ˆ fingerprintï¼ˆsize + mtimeï¼‰
    "source_mtime",      # æª”æ¡ˆæœ€å¾Œç•°å‹•æ™‚é–“
    "source_size",       # æª”æ¡ˆå¤§å°ï¼ˆbytesï¼‰
    "status",            # pending / success / failed
    "date",              # æ¸…åˆ°çš„ date_keyï¼ˆä¾‹å¦‚ 20251121ï¼‰
    "item",              # itemï¼ˆä¾‹å¦‚ ä¸‰å¤§æ³•äººè²·è³£è¶…æ—¥å ±ï¼‰
    "last_processed_at", # æˆ‘å€‘æˆåŠŸ/å¤±æ•—å¯«å…¥ DB çš„æ™‚é–“
]

def _calc_file_state(path: str) -> Dict[str, Any]:
    """
    çµ±ä¸€å–å¾— source æª”æ¡ˆçš„ç›®å‰ç‹€æ…‹ï¼š
    - hashï¼šç”¨ size + mtime çµ„åˆè€Œæˆï¼Œè¶³å¤ åˆ¤æ–·æ˜¯å¦ç•°å‹•
    - source_sizeï¼šæª”æ¡ˆå¤§å°ï¼ˆbytesï¼‰
    - source_mtimeï¼šæœ€å¾Œä¿®æ”¹æ™‚é–“ï¼ˆpandas Timestampï¼‰
    """
    p = Path(path)
    st = p.stat()
    size = st.st_size
    mtime = st.st_mtime  # floatï¼ˆç§’ï¼‰

    # fingerprintï¼šsize + æ•´æ•¸ mtime å­—ä¸²
    fp = f"{size}-{int(mtime)}"

    return {
        "hash": fp,
        "source_size": size,
        "source_mtime": pd.Timestamp.fromtimestamp(mtime),
    }


def _get_span_cfg(item: str, subitem: str) -> Optional[dict]:
    """
    ä¾åºå˜—è©¦ï¼š
      1) fields_span[item][subitem]
      2) fields_span[subitem]
    æœ‰å“ªå€‹å°±ç”¨å“ªå€‹ï¼›éƒ½æ²’æœ‰å›å‚³ Noneã€‚
    """
    by_item = (fields_span.get(item, {}) or {}).get(subitem)
    direct  = fields_span.get(subitem)
    return by_item or direct

def _is_partition_by_date_item(item: str) -> bool:
    """
    åˆ¤æ–·æ­¤ item æ˜¯å¦ç‚ºã€Œæ—¥é »ç‡ã€çš„æ—¥å ±è¡¨ï¼š
    - ä¾æ“š config.conf.collection[item]['freq']
    - åªè¦ freq æ˜¯ 'D' / 'd' æˆ–é¡ä¼¼ '1D'ï¼Œå°±è¦–ç‚ºæŒ‰ date åš partition è¦†å¯«
    """
    cfg = collection.get(item) or {}
    freq = cfg.get("freq")
    if freq is None:
        return False

    # çµ±ä¸€æˆå­—ä¸²åˆ¤æ–·ï¼Œé¿å…å¤§å°å¯«å•é¡Œæˆ– '1D' ä¹‹é¡å¯«æ³•
    s = str(freq).strip().upper()
    if s == "D":
        return True
    # å¦‚æœä½ æœªä¾†æƒ³æ”¯æ´ '1D'ã€'DAY' ä¹‹é¡ï¼Œä¹Ÿå¯ä»¥é †ä¾¿æ‰“é–‹ï¼š
    if s in {"1D", "DAY", "DAILY"}:
        return True

    return False

def _make_bucket_key(date_series: pd.Series, mode: str) -> pd.Series:
    """
    æŠŠ date æ¬„ä½è½‰æˆã€Œbucket keyã€ï¼Œä¾ mode å›å‚³å­—ä¸² Seriesï¼š
      - all     â†’ å…¨éƒ¨åŒä¸€ bucketï¼ˆä¸æ‡‰é€²ä¾†ï¼Œå‘¼å«ç«¯æœƒå…ˆç•¥éï¼‰
      - year    â†’ '2020'
      - quarter â†’ '2020Q1'
      - month   â†’ '2020-01'
      - day     â†’ '2020-01-31'
    """
    mode = (mode or "all").lower()

    if not pd.api.types.is_datetime64_any_dtype(date_series):
        # é˜²å‘†ï¼šå¦‚æœä¸æ˜¯ datetimeï¼Œå°±ç¡¬è½‰ä¸€æ¬¡
        date_series = pd.to_datetime(date_series)

    if mode == "year":
        return date_series.dt.strftime("%Y")

    if mode == "quarter":
        # to_period('Q') æœƒç”¢ç”Ÿé¡ä¼¼ '2020Q1'
        return date_series.dt.to_period("Q").astype(str)

    if mode == "month":
        return date_series.dt.strftime("%Y-%m")

    if mode == "day":
        return date_series.dt.strftime("%Y-%m-%d")

    # å…¶ä»–ï¼ˆå« allï¼‰å‘¼å«ç«¯ä¸æ‡‰é€²ä¾†ï¼›é€™è£¡ç›´æ¥çµ¦åŒä¸€å€‹ key
    return pd.Series(["ALL"] * len(date_series), index=date_series.index)

# ---- è‡ªè¨‚éŒ¯èª¤é¡ï¼Œè®“éŒ¯èª¤æƒ…å¢ƒæ›´æ¸…æ¥š ----
class DataCleanError(RuntimeError):
    def __init__(
        self,
        message: str,
        *,
        file: Optional[str] = None,
        item: Optional[str] = None,
        subitem: Optional[str] = None,
        column: Optional[str] = None,
        value: Any = None,
        value_type: Optional[str] = None,
        date: Optional[str] = None,
        hint: Optional[str] = None,
    ):
        parts = [message]
        ctx = []
        if file: ctx.append(f"file={file}")
        if item: ctx.append(f"item={item}")
        if subitem: ctx.append(f"subitem={subitem}")
        if column: ctx.append(f"column={column}")
        if value is not None: ctx.append(f"value={repr(value)}")
        if value_type: ctx.append(f"value_type={value_type}")
        if date: ctx.append(f"date={date}")
        if hint: ctx.append(f"hint={hint}")
        if ctx:
            parts.append(" | " + ", ".join(ctx))
        super().__init__("".join(parts))


# ---- å°å·¥å…· ----
def _ensure_dir(p: str) -> None:
    makedirs(p, exist_ok=True)


def _normalize_cols(cols: List[str]) -> List[str]:
    """æ¬„åæ­¸ä¸€ï¼šå…ˆ colname_dic æ˜ å°„ï¼Œå†ç§»é™¤ HTML æ–·è¡Œã€‚"""
    mapped = [colname_dic.get(c, c) for c in cols]
    cleaned = [safe_replace(c, "</br>", "") for c in mapped]
    return cleaned


def _list_source_pickles(root: str) -> pd.DataFrame:
    """
    åˆ—å‡º root ä¸‹æ‰€æœ‰ .pkl æª”ï¼›å›å‚³ DataFrame åŒ…å« columns: file, path, dirï¼ˆä¸Šå±¤è³‡æ–™å¤¾åï¼‰
    å„ªå…ˆç”¨ StevenTricks.PathWalk_df
    """
    df = PathWalk_df(root, [], ["log"], [".DS_Store","productlist"], [".pkl"])  # ä¾ä½ çš„æ…£ä¾‹
    # æœŸå¾…æœ‰ 'file', 'path', 'dir' æ¬„ï¼›è‹¥æ²’æœ‰å°±è£œ
    need_cols = {"file", "path", "dir"}
    have = set(df.columns)
    if not need_cols.issubset(have):
        # å˜—è©¦è£œé½Š
        if "path" not in df.columns:
            raise DataCleanError("PathWalk_df ç¼ºå°‘ path æ¬„")
        df["file"] = df["path"].map(lambda p: basename(p))
        df["dir"] = df["path"].map(lambda p: Path(p).parent.name)
    return df[["file", "path", "dir"]].copy()


def _load_job_state() -> pd.DataFrame:
    """
    å¾ dbpath_cleaned_log è¼‰å…¥ job_stateï¼š
    - è‹¥ä¸å­˜åœ¨ â†’ å›å‚³ç©º DataFrameï¼ˆå«å›ºå®šæ¬„ä½ï¼‰
    - è‹¥æ˜¯èˆŠç‰ˆ logï¼ˆset/list/DataFrameï¼‰â†’ è‡ªå‹•è£œé½Šæ¬„ä½
    """
    if not exists(dbpath_cleaned_log):
        return pd.DataFrame(columns=JOB_STATE_COLUMNS)

    obj = pickleio(path=dbpath_cleaned_log, mode="load")

    if isinstance(obj, pd.DataFrame):
        js = obj.copy()
        # è£œé½Šç¼ºå°‘æ¬„ä½
        for col in JOB_STATE_COLUMNS:
            if col not in js.columns:
                js[col] = pd.NA
        return js[JOB_STATE_COLUMNS]

    # èˆŠç‰ˆï¼šset/list åªè¨˜ file åç¨±
    if isinstance(obj, (set, list, tuple)):
        return pd.DataFrame(
            {"file": list(map(str, obj))},
            columns=JOB_STATE_COLUMNS,
        )

    # å…¶ä»–æœªçŸ¥æ ¼å¼ï¼šç•¶ä½œç©º
    return pd.DataFrame(columns=JOB_STATE_COLUMNS)


def _save_job_state(job_state: pd.DataFrame) -> None:
    """
    å°‡ job_state å­˜å› dbpath_cleaned_logã€‚
    ç¢ºä¿æ¬„ä½é †åºèˆ‡ JOB_STATE_COLUMNS ä¸€è‡´ã€‚
    """
    for col in JOB_STATE_COLUMNS:
        if col not in job_state.columns:
            job_state[col] = pd.NA
    job_state = job_state[JOB_STATE_COLUMNS]
    pickleio(path=dbpath_cleaned_log, data=job_state, mode="save")


def _upsert_job_state_row(
    job_state: pd.DataFrame,
    *,
    file: str,
    path: str,
    dir_name: str,
    date_key: Optional[str],
    item: Optional[str],
    state: Dict[str, Any],
) -> pd.DataFrame:
    """
    æ–°å¢æˆ–æ›´æ–°ä¸€ç­† job_state ç´€éŒ„ï¼ˆä»¥ path ç•¶å”¯ä¸€éµï¼‰ã€‚

    åƒæ•¸ï¼š
      - fileï¼šæª”åï¼ˆä¸å«è·¯å¾‘ï¼‰
      - pathï¼šå®Œæ•´è·¯å¾‘
      - dir_nameï¼šä¸Šå±¤è³‡æ–™å¤¾åï¼ˆTWSE é¡åˆ¥ï¼‰
      - date_keyï¼šæœ¬æ¬¡æ¸…ç†å¾—åˆ°çš„ dateï¼ˆè‹¥é‚„æ²’æ‹¿åˆ°å¯çµ¦ Noneï¼‰
      - itemï¼šTWSE item åç¨±ï¼ˆä¾‹å¦‚ ä¸‰å¤§æ³•äººè²·è³£è¶…æ—¥å ±ï¼Œè‹¥é‚„æ²’æ‹¿åˆ°å¯çµ¦ Noneï¼‰
      - stateï¼šè¦è¦†å¯«çš„æ¬„ä½ dictï¼Œä¾‹å¦‚ï¼š
          {"status": "pending", "hash": "...", "source_mtime": ts, ...}
    """
    if job_state.empty:
        idx = pd.Series([], dtype=bool)
    else:
        idx = (job_state["path"] == path)

    if not idx.any():
        # æ–°ç´€éŒ„
        row = {col: pd.NA for col in JOB_STATE_COLUMNS}
        row.update(
            {
                "file": file,
                "path": path,
                "dir": dir_name,
                "date": date_key,
                "item": item,
            }
        )
        row.update(state)

        # ğŸ‘‡ ä¿®æ­£é€™è£¡ï¼Œé¿å…ã€Œç©º DataFrame + concatã€é€ æˆ FutureWarning
        row_df = pd.DataFrame([row], columns=JOB_STATE_COLUMNS)

        if job_state.empty:
            # ç¬¬ä¸€æ¬¡ç›´æ¥ç”¨ row_df ç•¶èµ·å§‹ job_state
            job_state = row_df
        else:
            # å¾ŒçºŒæ‰ç”¨ concat ç–Šä¸Šå»
            job_state = pd.concat([job_state, row_df], ignore_index=True)

    else:
        # æ›´æ–°æ—¢æœ‰ç´€éŒ„ï¼ˆåŸä¾†é€™æ®µä¿æŒä¸å‹•ï¼‰
        for k, v in state.items():
            if k in job_state.columns:
                job_state.loc[idx, k] = v

        job_state.loc[idx, "file"] = file
        job_state.loc[idx, "path"] = path
        job_state.loc[idx, "dir"] = dir_name
        if date_key is not None:
            job_state.loc[idx, "date"] = date_key
        if item is not None:
            job_state.loc[idx, "item"] = item

    return job_state



# ---- è§£æ TWSE API çµæ§‹ â†’ å­è¡¨ dict list ----
def key_extract(dic: dict) -> list[dict]:
    """
    ä¾æ“šå…¨åŸŸ key_set å¾ raw dict æ“·å–å¤šå€‹ã€Œå­è¡¨ã€ç‰‡æ®µ (fields/data/title/...)ï¼Œ
    åŒæ™‚æ”¯æ´ï¼š
      - step == "main1"ï¼šå¸¶åºè™Ÿåˆ‡ç‰‡ï¼ˆå¦‚ fields, fields1, fields2, ...ï¼‰
      - å…¶ä»– stepï¼ˆå¦‚ "set1"ï¼‰ï¼šä¸€æ¬¡èšåˆ
      - raw["tables"]ï¼šè‹¥å­˜åœ¨ä¸”ç‚º listï¼Œé€ä¸€ç”¨ç›¸åŒè¦å‰‡æŠ½å–
    å›å‚³ï¼šlist[dict]ï¼Œä¾‹å¦‚ [{"fields":..., "data":..., "title":..., "groups":..., "notes":...}, ...]
    """
    if not isinstance(dic, dict):
        raise TypeError(f"key_extract() expects dict, got {type(dic).__name__}")

    out: list[dict] = []

    def _listify(x):
        # è¨­å®šå…è¨±å¯«æˆå­—ä¸²æˆ–æ¸…å–®ï¼›çµ±ä¸€è½‰æ¸…å–®
        return x if isinstance(x, (list, tuple)) else [x]

    def _find_first_key(container: dict, aliases: list[str]) -> tuple[str, bool]:
        """
        åœ¨ container è£¡ä¾åºæ‰¾ç¬¬ä¸€å€‹å­˜åœ¨çš„åˆ¥åéµï¼›å›å‚³ (å‘½ä¸­çš„éµå, æ˜¯å¦å‘½ä¸­)
        """
        for k in aliases:
            if k in container:
                return k, True
        return "", False

    def _extract_from_container(container: dict) -> list[dict]:
        """
        ä¾ key_set è¦å‰‡ï¼Œå¾å–®ä¸€ container (é€šå¸¸æ˜¯ raw æˆ– raw çš„ä¸€å€‹ table dict) æŠ½å‡ºå­è¡¨ã€‚
        """
        dicts: list[dict] = []
        for step, set_i in key_set.items():
            if not isinstance(set_i, dict):
                continue

            if step == "main1":
                # å®Œå…¨å¾©åˆ»ä½ åŸæœ¬çš„åœæ­¢æ¢ä»¶ï¼šcnt > 1 ä¸”æ²’å‘½ä¸­å°±åœæ­¢
                cnt = 0
                while True:
                    curr: dict = {}
                    for key_name, alias_list in set_i.items():
                        aliases = _listify(alias_list)
                        # cnt==0 ç”¨åŸåï¼›cnt>0 ç”¨ f"{alias}{cnt}"
                        candidates = [a if cnt == 0 else f"{a}{cnt}" for a in aliases]
                        hit_key, ok = _find_first_key(container, candidates)
                        if ok:
                            curr[key_name] = container[hit_key]

                    if curr:
                        dicts.append(curr)
                    elif cnt > 1:
                        # èˆ‡åŸç¢¼é‚è¼¯ç­‰åƒ¹ï¼šcnt>1 ä¸”ç„¡å‘½ä¸­ â†’ break
                        break

                    cnt += 1

            else:
                # é main1ï¼šåªåšä¸€æ¬¡èšåˆ
                curr: dict = {}
                for key_name, alias_list in set_i.items():
                    aliases = _listify(alias_list)
                    hit_key, ok = _find_first_key(container, aliases)
                    if ok:
                        curr[key_name] = container[hit_key]
                if curr:
                    dicts.append(curr)

        return dicts

    # 1) å…ˆå¾ raw æœ¬é«”æŠ½
    out.extend(_extract_from_container(dic))

    # 2) è‹¥ raw ä¸­é‚„æœ‰ tablesï¼ˆå¤šè¡¨ï¼‰ï¼Œé€ä¸€è™•ç†
    tables = dic.get("tables")
    if isinstance(tables, list):
        for t in tables:
            if isinstance(t, dict):
                out.extend(_extract_from_container(t))

    return out


# ---- å…©ç¨® DataFrame çµ„è£ ----
def frameup_safe(d: Dict[str, Any]) -> pd.DataFrame:
    """
    ç„¡ç¾¤çµ„æ¬„ä½ï¼šç›´æ¥ä»¥ d['fields'] å°é½Š d['data']ã€‚
    è‹¥ data çš„æ¬„æ•¸ > fieldsï¼Œè¶…å‡ºè€…ä¸Ÿæ£„ï¼ˆçµæ§‹å™ªéŸ³ï¼‰ï¼Œä½†ä¸ä¸Ÿåˆ—ã€‚
    """
    fields = list(d.get("fields",[]))
    rows = list(d.get("data",[]))
    if not fields or not rows:
        raise DataCleanError("frameup_safeï¼šç¼ºå°‘ fields æˆ– data")
    # æˆªæ–·æ¯åˆ—åˆ° len(fields)ï¼Œé¿å…é‡æ¬„ä½
    trimmed = [r[: len(fields)] for r in rows]
    df = pd.DataFrame(trimmed, columns=_normalize_cols(fields))
    return df


def data_cleaned_groups(d: Dict[str, Any], span_cfg: Dict[str, Any]) -> pd.DataFrame:
    """
    æœ‰ç¾¤çµ„æ¬„ä½ï¼ˆå¦‚ èè³‡/èåˆ¸ï¼‰ï¼š
    span_cfg ä¾‹ï¼š {"groups": [{"prefix":"èè³‡_", "size":5}, {"prefix":"èåˆ¸_","size":5}], "index": ["æ—¥æœŸ","ä»£è™Ÿ",...]}
    """
    fields = list(d.get("fields",[]))
    rows = list(d.get("data",[]))
    if not fields or not rows:
        raise DataCleanError("data_cleaned_groupsï¼šç¼ºå°‘ fields æˆ– data")
    groups = span_cfg.get("groups")
    if not groups:
        raise DataCleanError("data_cleaned_groupsï¼šspan_cfg ç¼ºå°‘ groups")
    # è¨ˆç®—æ¯ç¾¤çµ„æ¬„ä½æ•¸ï¼Œç”¢ç”Ÿç›®æ¨™æ¬„å
    start = 0
    col_names: List[str] = []
    for g in groups:
        size = int(g.get("size", 0))
        prefix = str(g.get("prefix", ""))
        if size <= 0:
            raise DataCleanError("span group size éæ­£æ•¸", value=size)
        end = start + size
        seg_fields = fields[start:end]
        seg_cols = [f"{prefix}{c}" for c in seg_fields]
        col_names.extend(seg_cols)
        start = end
    # ç å°¾ï¼ˆè‹¥ fields æ¯”ç¾¤çµ„ç¸½é•·é‚„é•·ï¼‰ï¼Œæˆ–ä¸è¶³ï¼ˆè£œç©ºæ¬„ï¼‰â†’ çš†ä»¥ã€Œä¸åˆªåˆ—ã€ç‚ºåŸå‰‡
    total = len(col_names)
    trimmed = [ (r[:total] + [None] * max(0, total - len(r))) for r in rows ]
    df = pd.DataFrame(trimmed, columns=_normalize_cols(col_names))
    return df


# ---- æœ€çµ‚æ¬„ä½æ¸…ç†ï¼ˆdrop/rename/numeric/dateï¼‰ ----
def finalize_dataframe(
    df: pd.DataFrame,
    *,
    item: str,
    subitem: str,
    date_key: str,
) -> pd.DataFrame:
    """
    ä¾ config è¦å‰‡ç”¢ç”Ÿæœ€çµ‚æ¸…æ´—å¾Œ DataFrameï¼›é‡åˆ°æœªçŸ¥æ¬„ä½/å‹åˆ¥å•é¡Œç›´æ¥ä¸Ÿ DataCleanErrorã€‚
    """
    # 1) ç§»é™¤ä¸éœ€è¦çš„æ¬„ï¼ˆä¾‹å¦‚ æ¼²è·Œ(+/-)ï¼‰
    df = df.drop(columns=dropcol, errors="ignore")
    # 2) æ¬„åèˆŠâ†’æ–°ï¼ˆç´°é …è¦å‰‡ï¼‰
    rename_cfg = transtonew_col.get(item,{}).get(subitem,{})
    if rename_cfg:
        df = df.rename(columns=rename_cfg)
    # 3) æ•¸å€¼æ¬„ä½è½‰æ›
    num_cfg = numericol.get(item,{}).get(subitem,{})
    df = safe_numeric_convert(df, num_cfg)

    # 4) è‹¥æ²’æœ‰ä»»ä½•æ—¥æœŸæ¬„ï¼Œè£œä¸€å€‹çµ±ä¸€ 'date'
    if "date" not in df.columns:
        df.insert(0, "date", date_key)

    # 5) æ—¥æœŸæ¬„ä½è½‰æ›ï¼ˆè‹¥æœ‰æŒ‡å®š datecolï¼‰
    date_cfg = datecol.get(item,{}).get(subitem,["date"])
    try:
        df = stringtodate(df, datecol=date_cfg, mode=4)
    except Exception as e:
        # å ±å‡ºç¬¬ä¸€å€‹å£å€¼ã€å‹åˆ¥
        bad = df
        raise DataCleanError(
            "æ—¥æœŸæ¬„ä½è½‰æ›å¤±æ•—",
            item=item, subitem=subitem, column=date_cfg,
            value=bad, value_type=type(bad).__name__,
            hint="è«‹è£œå…… changetype_stringtodate è¦å‰‡æˆ–å‰ç½®æ¸…ç†é‚è¼¯",
        ) from e


    # 6) æ¬„ä½é †åºå¾®æ•´ï¼šæŠŠå¸¸è¦‹éµæ”¾å‰é¢
    front = [c for c in ["date", "ä»£è™Ÿ","åç¨±"] if c in df.columns]
    rest = [c for c in df.columns if c not in front]
    df = df[front + rest]
    return df

# ---- å¯«å…¥è³‡æ–™åº« ----
def _db_path_for_item(item: str) -> str:
    _ensure_dir(dbpath_cleaned)
    return join(dbpath_cleaned, f"{item}")


def _write_to_db(
    df: pd.DataFrame,
    convert_mode: str = "upcast",
    *,
    item: str,
    subitem: str,
    bucket_mode: str = "all",     # â˜… æ–°å¢
) -> None:
    """
    é è¨­ PK è¦å‰‡ï¼š
      - åŒæ™‚æœ‰ 'ä»£è™Ÿ' èˆ‡ 'date' â†’ ['ä»£è™Ÿ','date']
      - åƒ…æœ‰ 'date' â†’ ['date']
      - å¦å‰‡ä¸è¨­ PKï¼ˆäº¤ç”± DBPkl è™•ç†ï¼‰
    """
    pk: List[str] = []
    if "ä»£è™Ÿ" in df.columns and "date" in df.columns:
        pk = ["ä»£è™Ÿ", "date"]
    elif "åç¨±" in df.columns and "date" in df.columns:
        pk = ["åç¨±", "date"]
    elif "date" in df.columns:
        pk = ["date"]

    db_path = _db_path_for_item(item)

    partition_by_date = "date" in df.columns and _is_partition_by_date_item(item)

    logger.debug(
        "å¯«å…¥ DBï¼š%s è¡¨=%s PK=%s partition_by_date=%s bucket_mode=%s",
        db_path,
        subitem,
        pk,
        partition_by_date,
        bucket_mode,
    )

    # ---- â˜… è‹¥æ˜¯æ—¥é »ç‡ + bucket_mode != all â†’ ä¾ bucket æ‹†è¡¨ ----
    if partition_by_date and bucket_mode.lower() != "all":
        # ç”¢ç”Ÿ bucket keyï¼ˆå­—ä¸²ï¼‰
        bucket_key = _make_bucket_key(df["date"], bucket_mode)

        # ä¸€å€‹ bucket å°æ‡‰ä¸€å€‹å¯¦éš› tableï¼Œä¾‹å¦‚ï¼š
        #   subitem='ä¸‰å¤§æ³•äººè²·è³£è¶…æ—¥å ±', bucket='2020-01'
        #   â†’ table_name='ä¸‰å¤§æ³•äººè²·è³£è¶…æ—¥å ±__2020-01'
        for b, df_chunk in df.groupby(bucket_key):
            table_name = f"{subitem}__{b}"

            logger.debug(
                "å¯«å…¥åˆ†æ¡¶è¡¨ï¼š%s/%sï¼ˆbucket=%s, rows=%dï¼‰",
                db_path,
                table_name,
                b,
                len(df_chunk),
            )

            # â˜… data æª”åç”¨ table_nameï¼ˆæœƒå¸¶ __2012 ç­‰ï¼‰ï¼Œ
            #   schema ä¸€å¾‹ç”¨ logical_table_name=subitem
            dbi = DBPkl(
                db_path,
                table_name,
                logical_table_name=subitem,
            )


            try:
                dbi.write_partition(
                    df_chunk,
                    convert_mode=convert_mode,
                    partition_cols=["date"],
                    primary_key=(pk if pk else None),
                )
            except Exception as e:
                # debug å€å¡Šç…§èˆŠï¼Œä½†æ³¨æ„ç”¨ table_name
                global DEBUG_LAST_DF, DEBUG_LAST_CONTEXT
                DEBUG_LAST_DF = df_chunk
                conflict = getattr(dbi, "schema_conflict", None)
                try:
                    dtypes = df_chunk.dtypes.astype(str).to_dict()
                except Exception:
                    dtypes = {}

                DEBUG_LAST_CONTEXT = {
                    "item": item,
                    "subitem": table_name,
                    "db_path": str(db_path),
                    "pk": pk,
                    "convert_mode": convert_mode,
                    "conflict": conflict,
                    "exception_type": type(e).__name__,
                    "exception_str": str(e),
                    "columns": list(df_chunk.columns),
                    "shape": tuple(df_chunk.shape),
                    "head": df_chunk.head(5),
                    "dtypes": dtypes,
                }
                if conflict:
                    logger.debug(f"[DB schema conflict] {conflict}")
                raise

        # åˆ†æ¡¶æ¨¡å¼ä¸‹ï¼Œé€™å€‹å‡½å¼åˆ°é€™è£¡å°±çµæŸï¼Œä¸å†èµ°ä¸‹é¢çš„ã€Œå–®ä¸€è¡¨ã€é‚è¼¯
        return


    # ---- â˜… å¦å‰‡ç¶­æŒåŸæœ¬å–®ä¸€è¡¨è¡Œç‚º ----
    dbi = DBPkl(db_path, subitem, logical_table_name=subitem)


    try:
        if partition_by_date:
            dbi.write_partition(
                df,
                convert_mode=convert_mode,
                partition_cols=["date"],
                primary_key=(pk if pk else None),
            )
        else:
            dbi.write_db(
                df,
                convert_mode=convert_mode,
                primary_key=(pk if pk else None),
            )

    except Exception as e:
        # åŸæœ¬ debug å€å¡ŠåŸå°ä¸å‹•

        DEBUG_LAST_DF = df
        conflict = getattr(dbi, "schema_conflict", None)
        try:
            dtypes = df.dtypes.astype(str).to_dict()
        except Exception:
            dtypes = {}

        DEBUG_LAST_CONTEXT = {
            "item": item,
            "subitem": subitem,
            "db_path": str(db_path),
            "pk": pk,
            "convert_mode": convert_mode,
            "conflict": conflict,
            "exception_type": type(e).__name__,
            "exception_str": str(e),
            "columns": list(df.columns),
            "shape": tuple(df.shape),
            "head": df.head(5),
            "dtypes": dtypes,
        }
        if conflict:
            logger.debug(f"[DB schema conflict] {conflict}")
        raise
        # === ç…§ä½ çš„ç­–ç•¥ï¼šé‡éŒ¯å°±åœ ===



# ---- æ¸…æ´—ä¸€å€‹æª”æ¡ˆï¼ˆä¸»æµç¨‹å­æ­¥é©Ÿï¼‰ ----
def _process_one_file(
    file_path: str,
    *,
    bucket_mode: str = "all",   # â˜… æ–°å¢
) -> Tuple[str, str, str]:
    """
    æ¸…æ´—å–®ä¸€ .pkl æª”ã€‚
    å›å‚³ï¼š(date_key, item, file_name)
    """
    file_name = basename(file_path)
    parentdir = Path(file_path).parent.name  # ä½œç‚º item
    logger.info(f"è™•ç†æª”æ¡ˆï¼š{file_name}ï¼ˆé¡åˆ¥={parentdir}ï¼‰")

    raw = pickleio(path=file_path, mode="load")
    if not isinstance(raw, dict):
        raise DataCleanError("åŸå§‹ pkl é dict çµæ§‹", file=file_name)

    base, _ = splitext(file_name)

    # å– crawler å–å¾—æ—¥
    try:
        date_key = raw.get("crawlerdic",{}).get("payload",{}).get("date")
    except Exception as e:
        raise DataCleanError("ç„¡æ³•å–å¾— crawler æ—¥æœŸ", file=file_name, item=parentdir, value=raw.get("crawlerdic")) from e

    # æ±ºå®šå…è¨±çš„å­è¡¨ï¼ˆæ¨™æº–åŒ–å¾Œï¼‰
    # ä»¥ crawler çš„ subtitle å„ªå…ˆï¼Œå¦å‰‡å– config.collection[item]['subtitle']
    subtitle_from_crawler = raw.get("crawlerdic",{}).get("subtitle")
    if isinstance(subtitle_from_crawler, list) and subtitle_from_crawler:
        subtitle_allowed = [colname_dic.get(x, x) for x in subtitle_from_crawler]
    else:
        subtitle_allowed = [colname_dic.get(x, x) for x in (collection.get(parentdir, {}).get("subtitle") or [parentdir])]

    if not subtitle_allowed:
        raise RuntimeError(
            f"subtitle_allowed ç‚ºç©ºï¼Œç„¡æ³•åˆ¤å®šæ¸…ç†ç›®æ¨™ï¼›"
            f"file={file_name}, item={parentdir}. "
            f"è«‹æª¢æŸ¥ crawlerdic.subtitle æˆ– config.collection['{parentdir}']['subtitle']"
        )
    # å–æ‰€æœ‰å­è¡¨ï¼ˆtitle, fields, dataï¼‰
    sub_tables = key_extract(raw)

    # å¦‚æœå®Œå…¨æŠ“ä¸åˆ°å­è¡¨ï¼Œå…ˆåˆ¤æ–·æ˜¯ã€Œæ²’è³‡æ–™æ—¥ã€é‚„æ˜¯ã€Œæ ¼å¼ç•°å¸¸ã€
    if not sub_tables:
        stat_msg = raw.get("stat") or raw.get("note") or ""

        # å…¸å‹æƒ…æ³ï¼šTWSE å›å‚³ã€Œå¾ˆæŠ±æ­‰ï¼Œæ²’æœ‰ç¬¦åˆæ¢ä»¶çš„è³‡æ–™!ã€æˆ–é¡ä¼¼å­—çœ¼
        if isinstance(stat_msg, str) and (
            "æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„è³‡æ–™" in stat_msg or
            "æŸ¥ç„¡è³‡æ–™" in stat_msg
        ):
            logger.warning(
                f"ç•¥éæª”æ¡ˆï¼ˆè©²æ—¥ç„¡è³‡æ–™ï¼‰ï¼šfile={file_name}, item={parentdir}, stat={stat_msg!r}"
            )
            # é€™é‚Šç›´æ¥ç•¶ä½œã€Œç©ºè³‡æ–™æ—¥ã€ï¼Œè®“æµç¨‹ç¹¼çºŒè·‘å…¶ä»–æª”æ¡ˆ
            return date_key, parentdir, file_name

        # å…¶ä»–æƒ…æ³ â†’ çœŸçš„æ‰¾ä¸åˆ°è³‡æ–™è¡¨ï¼Œç¶­æŒåŸæœ¬åš´æ ¼éŒ¯èª¤ç­–ç•¥
        raise DataCleanError(
            "æœªæ‰¾åˆ°ä»»ä½•å¯æ¸…ç†çš„å­è¡¨",
            file=file_name,
            item=parentdir,
            value=list(raw.keys()),  # å¤šçµ¦ä½  raw çš„ key å¹«åŠ©ä¹‹å¾Œ debug
        )

    for idx, d in enumerate(sub_tables, 1):
        title = d.get("title")
        fields = d.get("fields")
        data = d.get("data")
        if not fields or not data:
            logger.debug(f"ç•¥éå­è¡¨ï¼ˆç„¡è³‡æ–™ï¼‰ï¼štitle={title!r}")
            continue

        # æ¨™æº–åŒ–å­è¡¨åç¨±
        subitem = keyinstr(title, dic=colname_dic, lis=subtitle_allowed, default=str(title) if title is not None else parentdir)

        # éé æœŸå­è¡¨ï¼šè·³éï¼ˆä¸æ˜¯éŒ¯èª¤ï¼‰
        if subitem not in subtitle_allowed:
            logger.debug(f"ç•¥éå­è¡¨ï¼ˆä¸åœ¨å…è¨±æ¸…å–®ï¼‰ï¼štitle={title!r}, æ¨™æº–å={subitem!r}")
            continue

        logger.debug(f"æ¸…ç†å­è¡¨ï¼š{subitem}ï¼ˆåŸ title={title!r}ï¼‰")

        # çµ„è£ DataFrameï¼ˆç¾¤çµ„ or å¹³é¢ï¼‰
        try:
            span_cfg = _get_span_cfg(parentdir, subitem)

            if span_cfg:
                df0 = data_cleaned_groups({"fields": fields, "data": data}, span_cfg)
            else:
                df0 = frameup_safe({"fields": fields, "data": data})

            # æœ€çµ‚è¦ç¯„åŒ–ï¼ˆdrop/rename/numeric/dateï¼‰
            df1 = finalize_dataframe(df0, item=parentdir, subitem=subitem, date_key=date_key)

        except DataCleanError:
            # ç›´æ¥å¾€å¤–æ‹‹ï¼ˆä½ è¦å®šé‡éŒ¯ä¸­æ–·ï¼‰
            raise
        except Exception as e:
            # åŒ…è£æˆ DataCleanErrorï¼Œé™„åŠ æ›´å¤šä¸Šä¸‹æ–‡
            raise DataCleanError(
                "å­è¡¨æ¸…ç†å¤±æ•—",
                file=file_name, item=parentdir, subitem=subitem, date=date_key,
                hint="è«‹æª¢æŸ¥ fields_span/dropcol/transtonew_col/numericol/datecol èˆ‡åŸå§‹è³‡æ–™æ˜¯å¦ä¸€è‡´",
            ) from e

        # å¯«å…¥ DBï¼ˆæ¯å€‹å­è¡¨ä¸€å¼µè¡¨ï¼‰
        _write_to_db(
            df1,
            item=parentdir,
            subitem=subitem,
            bucket_mode=bucket_mode,
        )
    return date_key, parentdir, file_name


# ---- æ¸…æ´—æµç¨‹ï¼ˆå¯è¢« import å‘¼å«ï¼‰ ----

def _process_twse_data_impl(
            cols: Optional[List[str]] = None,
            max_files_per_run: Optional[int] = None,# æ¯è¼ªæœ€å¤šè™•ç†å¹¾å€‹ã€Œå¯¦éš›æ¸…ç†ã€çš„æª”æ¡ˆ
            bucket_mode: str = "all",  # â˜… æ–°å¢
    ) -> int:
    """
    çœŸæ­£åŸ·è¡Œæ¸…ç†é‚è¼¯çš„å…§éƒ¨å‡½å¼ã€‚

    å›å‚³å€¼ï¼š
        æœ¬è¼ªã€Œå¯¦éš›æœ‰åŸ·è¡Œ _process_one_fileã€çš„æª”æ¡ˆæ•¸ï¼ˆç•¥éçš„ä¸ç®—ï¼‰ã€‚

    æ³¨æ„ï¼šé€™è£¡å‡è¨­ dbpath_cleaned / dbpath_cleaned_log å·²ç¶“æ˜¯ã€Œè¦å¯«å…¥çš„é‚£å€‹è·¯å¾‘ã€
          ï¼ˆå¯èƒ½æ˜¯ iCloudï¼Œå¯èƒ½æ˜¯æœ¬æ©Ÿ stagingï¼Œç”±å¤–å±¤è² è²¬æ±ºå®šï¼‰ã€‚
    """
    _ensure_dir(dbpath_cleaned)

    # 1) è¼‰å…¥ job_state
    job_state = _load_job_state()

    # 2) åˆ—å‡ºæ‰€æœ‰ source pkl æª”
    files_df = _list_source_pickles(dbpath_source)

    # è‹¥æœ‰æŒ‡å®šè¦æ¸…çš„é¡åˆ¥ï¼Œå…ˆéæ¿¾
    if cols:
        files_df = files_df[files_df["dir"].isin(cols)].copy()

    total_files = len(files_df)
    if files_df.empty:
        logger.info("æ‰¾ä¸åˆ°ä»»ä½•å¾…è™•ç†çš„ source æª”æ¡ˆã€‚")
        return 0

    logger.info(f"å¾…æª¢æŸ¥æª”æ¡ˆæ•¸ï¼š{total_files}")

    # === é€²åº¦çµ±è¨ˆç”¨ ===
    processed = 0         # æœ¬è¼ªå¯¦éš›æœ‰æ¸…ç†å¹¾æª”
    start_time = datetime.now()

    # 3) é€æª”æ±ºå®šï¼šç•¥é / é‡è·‘
    for scanned_idx, (_, row) in enumerate(files_df.iterrows(), start=1):

        # è‹¥æœ‰è¨­å®š max_files_per_runï¼Œä¸”å·²é”ä¸Šé™ â†’ æå‰çµæŸæœ¬è¼ª
        if max_files_per_run is not None and processed >= max_files_per_run:
            logger.info(
                "å·²é”æœ¬è¼ªè™•ç†ä¸Šé™ %d æª”ï¼Œæœ¬è¼ªæå‰çµæŸï¼ˆå¯¦éš›è™•ç† %d æª”ï¼Œæƒæåˆ°ç¬¬ %d æª” / ç¸½æª”æ•¸ %dï¼‰ã€‚",
                max_files_per_run,
                processed,
                scanned_idx - 1,
                total_files,
            )
            break

        file_path = row["path"]
        file_name = row["file"]
        dir_name  = row["dir"]

        # 3-1) å–å¾— source ç•¶å‰ç‹€æ…‹ï¼ˆsize / mtime / hashï¼‰
        state_now = _calc_file_state(file_path)
        fp_now    = state_now["hash"]
        mtime_now = state_now["source_mtime"]
        size_now  = state_now["source_size"]

        # 3-2) æ‰¾å‡º job_state æ—¢æœ‰ç´€éŒ„
        rec_idx = (job_state["path"] == file_path) if not job_state.empty else pd.Series([], dtype=bool)
        rec = job_state.loc[rec_idx].iloc[0] if rec_idx.any() else None

        # 3-3) åˆ¤æ–·æ˜¯å¦å¯ä»¥å®‰å…¨ç•¥é
        if rec is not None:
            rec_status = rec.get("status")
            rec_hash   = rec.get("hash")
            rec_mtime  = rec.get("source_mtime")

            # status = success ä¸” hash/mtime å®Œå…¨ä¸€è‡´ â†’ ç•¶ä½œæ²’è®ŠåŒ–ï¼Œç•¥é
            if (
                rec_status == "success"
                and pd.notna(rec_hash)
                and rec_hash == fp_now
                and pd.notna(rec_mtime)
                and pd.Timestamp(rec_mtime) == mtime_now
            ):
                logger.debug(f"ç•¥éæª”æ¡ˆï¼ˆsource æœªè®Šæ›´ï¼‰ï¼š{file_name}")
                continue

            # status æ˜¯ success ä½† hash/mtime æ”¹è®Š â†’ é˜²å‘†ï¼šæ”¹æˆ pending
            if rec_status == "success" and (
                rec_hash != fp_now
                or (pd.notna(rec_mtime) and pd.Timestamp(rec_mtime) != mtime_now)
            ):
                logger.warning(
                    "åµæ¸¬åˆ° source åœ¨ä¸Šæ¬¡æˆåŠŸæ¸…ç†å¾Œæœ‰è®Šæ›´ï¼Œæ¨™è¨˜ç‚º pendingï¼šfile=%s, old_mtime=%s, new_mtime=%s",
                    file_name,
                    rec_mtime,
                    mtime_now,
                )
                job_state.loc[rec_idx, "status"] = "pending"

        # 3-4) é€²å…¥æ¸…ç†å‰ï¼Œå…ˆæ¨™è¨˜ pending
        job_state = _upsert_job_state_row(
            job_state,
            file=file_name,
            path=file_path,
            dir_name=dir_name,
            date_key=None,
            item=None,
            state={
                "status": "pending",
                "hash": fp_now,
                "source_mtime": mtime_now,
                "source_size": size_now,
            },
        )
        _save_job_state(job_state)

        # 3-5) å¯¦éš›åŸ·è¡Œæ¸…ç†
        try:
            date_key, item, cleaned_file_name = _process_one_file(
                file_path,
                bucket_mode=bucket_mode,
            )
        except Exception as e:
            # æ¨™è¨˜ç‚º failed
            job_state = _upsert_job_state_row(
                job_state,
                file=file_name,
                path=file_path,
                dir_name=dir_name,
                date_key=None,
                item=None,
                state={
                    "status": "failed",
                    "hash": fp_now,
                    "source_mtime": mtime_now,
                    "source_size": size_now,
                    "last_processed_at": pd.Timestamp.utcnow(),
                },
            )
            _save_job_state(job_state)
            logger.error(f"è™•ç†ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            # æŒ‰ä½ çš„ç­–ç•¥ï¼šç›´æ¥æ•´é«”ä¸­æ­¢
            raise

        # 3-6) æ¸…ç†æˆåŠŸ â†’ æ›´æ–° job_state
        job_state = _upsert_job_state_row(
            job_state,
            file=file_name,
            path=file_path,
            dir_name=dir_name,
            date_key=date_key,
            item=item,
            state={
                "status": "success",
                "hash": fp_now,
                "source_mtime": mtime_now,
                "source_size": size_now,
                "last_processed_at": pd.Timestamp.utcnow(),
            },
        )
        _save_job_state(job_state)

        processed += 1
        elapsed = (datetime.now() - start_time).total_seconds()
        avg_sec = elapsed / processed if processed else 0.0

        logger.info(
            "å®Œæˆï¼š%sï¼ˆdate=%s, item=%sï¼‰ï½œæœ¬è¼ªå·²è™•ç† %d æª” / ç¸½æª”æ•¸ %dï¼ˆæƒæåˆ°ç¬¬ %d æª”ï¼‰ï¼Œå¹³å‡è€—æ™‚ %.1f ç§’/æª”ã€‚",
            cleaned_file_name,
            date_key,
            item,
            processed,
            total_files,
            scanned_idx,
            avg_sec,
        )

    return processed

def process_twse_data(
    cols: Optional[List[str]] = None,
    *,
    storage_mode: str = "cloud",     # "cloud" / "cloud_staging" / "local"
    batch_size: Optional[int] = None,
    bucket_mode: str = "all",
) -> None:
    """
    TWSE æ¸…ç†ä¸»å…¥å£ã€‚

    cols:
        è¦æ¸…å“ªå¹¾å€‹ itemï¼ˆå¦‚ ["ä¸‰å¤§æ³•äººè²·è³£è¶…æ—¥å ±"]ï¼‰ï¼ŒNone å‰‡æ¸…å…¨éƒ¨ã€‚

    storage_mode:
        - "cloud"         : ç›´æ¥ç”¨é›²ç«¯ db_rootï¼ˆconfig.paths çš„ db_rootï¼‰
        - "cloud_staging" : é›²ç«¯ + æœ¬æ©Ÿæš«å­˜ï¼ˆå…ˆæŠŠ cleaned æ•´åŒ…æ‹‰åˆ°æœ¬æ©Ÿè™•ç†ï¼Œå†åŒæ­¥å›é›²ç«¯ï¼‰
        - "local"         : å®Œå…¨åªç”¨æœ¬æ©Ÿ rootï¼ˆconfig.paths çš„ db_local_rootï¼‰ï¼Œä¸ç¢°é›²ç«¯

    batch_size:
        - åƒ…åœ¨ "cloud_staging" æ¨¡å¼æœ‰æ•ˆï¼Œæ¯ä¸€è¼ªæœ€å¤šè™•ç†å¹¾å€‹æª”æ¡ˆã€‚
          ä¾‹å¦‚ 500 è¡¨ç¤ºæ¯æ¬¡ staging åªè™•ç† 500 å€‹ source æª”ï¼Œè™•ç†å®ŒåŒæ­¥å›é›²ç«¯ï¼Œå†ä¸‹è¼‰ä¸‹ä¸€æ‰¹ã€‚
          None å‰‡è¦–ç‚ºã€Œä¸€æ¬¡æ¸…åˆ°åº•ã€ã€‚
    """
    global dbpath_source, dbpath_cleaned, dbpath_cleaned_log

    storage_mode = (storage_mode or "cloud").lower()
    if storage_mode not in {"cloud", "cloud_staging", "local"}:
        raise ValueError(f"storage_mode å¿…é ˆæ˜¯ 'cloud' / 'cloud_staging' / 'local'ï¼Œç›®å‰ç‚ºï¼š{storage_mode!r}")

    logger.info(
        "process_twse_data å•Ÿå‹•ï¼šstorage_mode=%s, bucket_mode=%s, dbpath_source(åˆå§‹)=%s, dbpath_cleaned(åˆå§‹)=%s",
        storage_mode,
        bucket_mode,
        dbpath_source,
        dbpath_cleaned,
    )

    # å…ˆå‚™ä»½ã€Œç›®å‰ã€çš„ active è·¯å¾‘ï¼ˆé€šå¸¸æ˜¯é›²ç«¯ï¼‰
    orig_source = dbpath_source
    orig_cleaned = dbpath_cleaned
    orig_cleaned_log = dbpath_cleaned_log

    # ---------- æƒ…å¢ƒ Aï¼šå®Œå…¨æœ¬æ©Ÿæ¨¡å¼ ----------
    if storage_mode == "local":
        # åˆ‡æ›æˆã€Œæœ¬æ©Ÿã€è·¯å¾‘
        dbpath_source = LOCAL_DBPATH_SOURCE
        dbpath_cleaned = LOCAL_DBPATH_CLEANED
        dbpath_cleaned_log = LOCAL_DBPATH_CLEANED_LOG

        _ensure_dir(str(dbpath_source))
        _ensure_dir(str(dbpath_cleaned))

        logger.info(
            "é€²å…¥ LOCAL æ¨¡å¼ï¼šsource=%s, cleaned=%s",
            dbpath_source,
            dbpath_cleaned,
        )

        try:
            _process_twse_data_impl(
                cols,
                bucket_mode=bucket_mode,
                max_files_per_run=batch_size,
            )
        finally:
            # ä¸ç®¡æœ‰æ²’æœ‰å‡ºéŒ¯ï¼Œéƒ½æŠŠè·¯å¾‘é‚„åŸ
            dbpath_source = orig_source
            dbpath_cleaned = orig_cleaned
            dbpath_cleaned_log = orig_cleaned_log

        return

    # ---------- æƒ…å¢ƒ B / Cï¼šä»¥é›²ç«¯ç‚ºä¸» ----------
    # å…ˆæŠŠ active è·¯å¾‘åˆ‡å›ã€Œé›²ç«¯ç‰ˆæœ¬ã€
    dbpath_source = CLOUD_DBPATH_SOURCE
    dbpath_cleaned = CLOUD_DBPATH_CLEANED
    dbpath_cleaned_log = CLOUD_DBPATH_CLEANED_LOG

    # B-1ï¼šç´”é›²ç«¯ï¼ˆèˆŠçš„ã€Œä¸ç”¨ stagingã€ï¼‰
    if storage_mode == "cloud":
        logger.info(
            "é€²å…¥ CLOUD (no staging) æ¨¡å¼ï¼šsource=%s, cleaned=%s",
            dbpath_source,
            dbpath_cleaned,
        )
        try:
            _process_twse_data_impl(
                cols,
                bucket_mode=bucket_mode,
                max_files_per_run=batch_size,
            )
        finally:
            dbpath_source = orig_source
            dbpath_cleaned = orig_cleaned
            dbpath_cleaned_log = orig_cleaned_log
        return

    # B-2ï¼šé›²ç«¯ + æœ¬æ©Ÿ stagingï¼ˆä½ åŸæœ¬çš„ use_local_db_staging=True æ¨¡å¼ï¼‰
    # ç­‰åŒä»¥å‰çš„ç¨‹å¼ï¼Œä½†é‚è¼¯æ¬åˆ°é€™è£¡è€Œä¸”æ›´æ˜ç¢º
    if batch_size is None:
        batch_size = 10_000_000  # ä¸€æ¬¡æ¸…åˆ°åº•

    target_cleaned: Path = CLOUD_DBPATH_CLEANED
    staging_root: Path = db_local_root

    batch_no = 0
    while True:
        batch_no += 1
        logger.info("===== é–‹å§‹ staging batch %dï¼Œbatch_size=%d =====", batch_no, batch_size)

        # staging_path æœƒï¼š
        # 1) æŠŠã€Œé›²ç«¯ cleanedã€æ•´å€‹è¤‡è£½åˆ° staging_root ä¸‹æŸå€‹ staging_xxx/cleaned è³‡æ–™å¤¾
        # 2) yield æœ¬æ©Ÿ cleaned çš„è·¯å¾‘
        # 3) é›¢é–‹ with æ™‚æŠŠæœ¬æ©ŸçµæœåŒæ­¥å›é›²ç«¯ï¼Œä¸¦æŠŠ staging_xxx åˆªæ‰
        with staging_path(target_cleaned, enable=True, staging_root=staging_root) as local_cleaned:
            try:
                dbpath_cleaned = local_cleaned
                dbpath_cleaned_log = local_cleaned / "log.pkl"

                logger.info(
                    "cloud_staging æ¨¡å¼ï¼šæœ¬è¼ªåœ¨æœ¬æ©Ÿ cleaned=%s ä¸Šè™•ç†",
                    dbpath_cleaned,
                )

                processed = _process_twse_data_impl(
                    cols,
                    bucket_mode=bucket_mode,
                    max_files_per_run=batch_size,
                )
            finally:
                # æ¢å¾©æˆé›²ç«¯ cleaned
                dbpath_cleaned = CLOUD_DBPATH_CLEANED
                dbpath_cleaned_log = CLOUD_DBPATH_CLEANED_LOG

        if processed == 0:
            logger.info("æ²’æœ‰å¾…è™•ç†æª”æ¡ˆï¼Œstaging è¿´åœˆçµæŸã€‚")
            break

        logger.info("===== staging batch %d å®Œæˆï¼Œæœ¬è¼ªè™•ç† %d å€‹æª”æ¡ˆ =====", batch_no, processed)

    # æœ€å¾Œä¿éšªå†æŠŠ active è·¯å¾‘æ¢å¾©åˆ°åŸæœ¬ç‹€æ…‹
    dbpath_source = orig_source
    dbpath_cleaned = orig_cleaned
    dbpath_cleaned_log = orig_cleaned_log


# ---- CLI ----
def _parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:
    p = argparse.ArgumentParser(description="TWSE è³‡æ–™æ¸…ç†å™¨ï¼ˆåˆä½µç‰ˆï¼‰")
    p.add_argument(
        "--col",
        nargs="*",
        help="æŒ‡å®šè¦æ¸…ç†çš„é¡åˆ¥ï¼ˆè³‡æ–™å¤¾å/collection keyï¼‰ï¼Œé è¨­å…¨æ¸…",
    )
    p.add_argument(
        "--storage-mode",
        type=str,
        default="cloud",
        choices=["cloud", "cloud_staging", "local"],
        help="è³‡æ–™å„²å­˜æ¨¡å¼ï¼šcloud=ç›´æ¥ç”¨é›²ç«¯ï¼›cloud_staging=é›²ç«¯+æœ¬æ©Ÿæš«å­˜ï¼›local=å®Œå…¨åªç”¨æœ¬æ©Ÿ db_local_rootã€‚",
    )

    p.add_argument(
        "--batch-size",
        type=int,
        default=None,
        help="æ¯ä¸€è¼ª staging è¦è™•ç†çš„æœ€å¤§æª”æ¡ˆæ•¸ï¼ˆä¾‹å¦‚ 500ï¼‰",
    )
    p.add_argument(
        "--bucket-mode",
        choices=["all", "year", "quarter", "month", "day"],
        default="all",
        help="æ—¥æœŸåˆ†æ¡¶æ¨¡å¼ï¼šall=æ•´æª”ä¸€å€‹è¡¨ã€year=æ¯å¹´ä¸€è¡¨ã€quarter=æ¯å­£ä¸€è¡¨ã€month=æ¯æœˆä¸€è¡¨ã€day=æ¯æ—¥ä¸€è¡¨",
    )
    args, unknown = p.parse_known_args(argv)
    if unknown:
        logger.debug("å¿½ç•¥æœªè­˜åˆ¥åƒæ•¸ï¼š%s", unknown)
    return args

def main(argv: Optional[List[str]] = None) -> None:
    args = _parse_args(argv)
    process_twse_data(
        cols=args.col,
        storage_mode=args.storage_mode,
        batch_size=args.batch_size,
        bucket_mode=args.bucket_mode,
    )


if __name__ == "__main__":
    # è®“ _parse_args è‡ªå·±å»è™•ç† sys.argvï¼ˆå« PyCharm çš„åƒåœ¾åƒæ•¸ï¼‰
    main()
#
# raw = {
#     "fields": ["A","B"],
#     "data": [[1,2]],
#     "title": "ä¸»è¡¨",
#     "groups": None,
#     "tables": [
#         {"fields1": ["X","Y"], "data1": [[9,8]], "subtitle": "å­è¡¨ä¸€"},
#         {"creditFields": ["C1","C2"], "creditList": [[3,4]], "creditTitle": "ä¿¡ç”¨è¡¨"},
#     ],
# }
# # ä½ çš„ key_set å¦‚é¡Œ
# lst = key_extract(raw)
# for i, d in enumerate(lst, 1):
#     print(i, d.keys())  # æ‡‰èƒ½çœ‹åˆ° fields/data/title/groups/notes ç­‰éµä¾è¦å‰‡è¢«æŠ½å‡º
#
#
#
#
# raw = pickleio(path=r"/Users/stevenhsu/Library/Mobile Documents/com~apple~CloudDocs/warehouse/stock/twse/source/ä¸‰å¤§æ³•äººè²·è³£è¶…æ—¥å ±/ä¸‰å¤§æ³•äººè²·è³£è¶…æ—¥å ±_2023-09-25.pkl", mode="load")
# raw1 = pickleio(path=r"/Users/stevenhsu/Library/Mobile Documents/com~apple~CloudDocs/warehouse/stock/twse/cleaned/ä¸‰å¤§æ³•äººè²·è³£è¶…æ—¥å ±/ä¸‰å¤§æ³•äººè²·è³£è¶…æ—¥å ±.pkl", mode="load")
# raw2 = pickleio(path=r"/Users/stevenhsu/Library/Mobile Documents/com~apple~CloudDocs/warehouse/stock/twse/cleaned/ä¸‰å¤§æ³•äººè²·è³£è¶…æ—¥å ±/ä¸‰å¤§æ³•äººè²·è³£è¶…æ—¥å ±_schema.pkl", mode="load")